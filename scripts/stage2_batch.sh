#!/bin/bash
#
# stage2_batch.sh — Batch SWE-Builder (Stage 2) for all repos from pipeline.sh
#
# This script finds all `instances_versions.jsonl` files generated by data_collection/pipeline.sh
# and runs SWE-Builder (app/main.py swe-bench) on each repository to generate Docker environments.
#
# Usage:
#   bash stage2_batch.sh --language Python --model gpt-4.1-mini
#   bash stage2_batch.sh --language Python --model gpt-4.1-mini --start-from 5 --end-at 10
#   bash stage2_batch.sh --language Python --model gpt-4.1-mini --num-processes 5 --conv-round-limit 5
#   bash stage2_batch.sh --language Python --model gpt-4.1-mini --repo-filter "OpenHands,autogen"
#   bash stage2_batch.sh --language Python --model gpt-4.1-mini --skip-existing
#
set -euo pipefail

# ============================================================
# 1. Default Configuration
# ============================================================
LANGUAGE="Python"                          # Programming language (Python, Java, JS, etc.)
MODEL="litellm-generic-openai/minimax-m2.1"  # LLM model for SWE-Builder
NUM_PROCESSES=10                           # Number of parallel processes for SWE-Builder
MODEL_TEMPERATURE=0.2                      # Model temperature
CONV_ROUND_LIMIT=10                        # Conversation round limit for agent refinement
TESTBED_DIR="testbed"                      # Temporary directory for environment setup
OUTPUT_BASE_DIR="output"                   # Base output directory
DATA_ROOT="data_collection/collect/data"   # Data root (relative to project root)
START_FROM=0                               # Start from repo index (0-indexed)
END_AT=-1                                  # End at repo index (exclusive, -1=all)
REPO_FILTER=""                             # Comma-separated repo names to filter (e.g., "OpenHands,autogen")
SKIP_EXISTING=true                        # Skip repos that already have results
DISABLE_MEMORY_POOL=false                  # Disable memory pool optimization

# API Configuration (override if needed)
export OPENAI_API_BASE=https://siflow-longmen.siflow.cn/siflow/longmen/skyinfer/skliu/minimax-litellm/v1/4000
export OPENAI_API_BASE_URL=$OPENAI_API_BASE  # Some tools read this variable
export OPENAI_API_KEY=EMPTY
export OPENAI_KEY=$OPENAI_API_KEY  # Some tools read this variable

# ============================================================
# 2. Parse arguments
# ============================================================
while [[ $# -gt 0 ]]; do
    case "$1" in
        --language)           LANGUAGE="$2";            shift 2 ;;
        --model)              MODEL="$2";               shift 2 ;;
        --num-processes)      NUM_PROCESSES="$2";       shift 2 ;;
        --model-temperature)  MODEL_TEMPERATURE="$2";   shift 2 ;;
        --conv-round-limit)   CONV_ROUND_LIMIT="$2";    shift 2 ;;
        --testbed-dir)        TESTBED_DIR="$2";         shift 2 ;;
        --output-base-dir)    OUTPUT_BASE_DIR="$2";     shift 2 ;;
        --data-root)          DATA_ROOT="$2";           shift 2 ;;
        --start-from)         START_FROM="$2";          shift 2 ;;
        --end-at)             END_AT="$2";              shift 2 ;;
        --repo-filter)        REPO_FILTER="$2";         shift 2 ;;
        --skip-existing)      SKIP_EXISTING=true;       shift ;;
        --disable-memory-pool) DISABLE_MEMORY_POOL=true; shift ;;
        -h|--help)
            echo "Usage: bash stage2_batch.sh [OPTIONS]"
            echo ""
            echo "Options:"
            echo "  --language LANG               Programming language (default: Python)"
            echo "  --model MODEL                 LLM model for SWE-Builder (default: litellm-generic-openai/minimax-m2.1)"
            echo "  --num-processes N             Parallel processes (default: 10)"
            echo "  --model-temperature T         Model temperature (default: 0.2)"
            echo "  --conv-round-limit N          Max conversation rounds (default: 10)"
            echo "  --testbed-dir DIR             Testbed directory (default: testbed)"
            echo "  --output-base-dir DIR         Output base directory (default: output)"
            echo "  --data-root DIR               Data root directory (default: data_collection/collect/data)"
            echo "  --start-from IDX              Start from repo index (0-indexed, default: 0)"
            echo "  --end-at IDX                  End at repo index (exclusive, -1=all, default: -1)"
            echo "  --repo-filter REPOS           Filter repos (comma-separated, e.g., 'OpenHands,autogen')"
            echo "  --skip-existing               Skip repos with existing results"
            echo "  --disable-memory-pool         Disable memory pool optimization"
            echo "  -h, --help                    Show this help"
            exit 0 ;;
        *) echo "Unknown option: $1"; exit 1 ;;
    esac
done

# ============================================================
# 3. Check environment
# ============================================================
if [ -z "${OPENAI_API_BASE:-}" ] && [ -z "${OPENAI_API_BASE_URL:-}" ]; then
    echo "Warning: OPENAI_API_BASE and OPENAI_API_BASE_URL are not set."
    echo "Please set one of them before running this script."
    echo ""
    echo "Example:"
    echo "  export OPENAI_API_BASE=https://api.openai.com/v1"
    echo "  export OPENAI_API_KEY=sk-xxx"
    echo ""
    read -p "Continue anyway? (y/n) " -n 1 -r
    echo
    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
        exit 1
    fi
fi

if [ -z "${OPENAI_API_KEY:-}" ] && [ -z "${OPENAI_KEY:-}" ]; then
    echo "Warning: OPENAI_API_KEY and OPENAI_KEY are not set."
    read -p "Continue anyway? (y/n) " -n 1 -r
    echo
    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
        exit 1
    fi
fi

# ============================================================
# 4. Setup paths and Python environment
# ============================================================
SCRIPT_DIR="$(cd "$(dirname "$0")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/.." && pwd)"
cd "$PROJECT_ROOT"
export PYTHONPATH="$PROJECT_ROOT:${PYTHONPATH:-}"

LANG_LOWER=$(echo "$LANGUAGE" | tr '[:upper:]' '[:lower:]')
DATA_DIR="$PROJECT_ROOT/$DATA_ROOT/$LANG_LOWER"

# ============================================================
# 5. Find all instances_versions.jsonl files
# ============================================================
echo ""
echo "=========================================="
echo "  Stage 2 Batch Processing"
echo "=========================================="
echo "  Language:           $LANGUAGE"
echo "  Model:              $MODEL"
echo "  Data directory:     $DATA_DIR"
echo "  Output directory:   $OUTPUT_BASE_DIR/$LANG_LOWER"
echo "  Num processes:      $NUM_PROCESSES"
echo "  Conv round limit:   $CONV_ROUND_LIMIT"
echo "=========================================="
echo ""

if [ ! -d "$DATA_DIR" ]; then
    echo "Error: Data directory not found: $DATA_DIR"
    echo "Please run data_collection/pipeline.sh first to generate data."
    exit 1
fi

echo "Scanning for instances_versions.jsonl files in $DATA_DIR ..."

mapfile -t ALL_INSTANCE_FILES < <(find "$DATA_DIR" -name "instances_versions.jsonl" -type f | sort)

if [ ${#ALL_INSTANCE_FILES[@]} -eq 0 ]; then
    echo "Error: No instances_versions.jsonl files found in $DATA_DIR"
    echo "Please run data_collection/pipeline.sh first."
    exit 1
fi

echo "Found ${#ALL_INSTANCE_FILES[@]} repositories with instances_versions.jsonl"
echo ""

# ============================================================
# 6. Apply repo filter if specified
# ============================================================
if [ -n "$REPO_FILTER" ]; then
    IFS=',' read -ra FILTER_REPOS <<< "$REPO_FILTER"
    FILTERED_FILES=()

    for file in "${ALL_INSTANCE_FILES[@]}"; do
        for filter_repo in "${FILTER_REPOS[@]}"; do
            if [[ "$file" == *"$filter_repo"* ]]; then
                FILTERED_FILES+=("$file")
                break
            fi
        done
    done

    ALL_INSTANCE_FILES=("${FILTERED_FILES[@]}")
    echo "Applied repo filter: $REPO_FILTER"
    echo "Filtered to ${#ALL_INSTANCE_FILES[@]} repositories"
    echo ""
fi

# ============================================================
# 7. Build repo list with metadata
# ============================================================
declare -a REPO_LIST          # Array of repo names (e.g., "OpenHands/OpenHands")
declare -a INSTANCE_FILES     # Array of instance file paths
declare -a OUTPUT_DIRS        # Array of output directories
declare -a INSTANCE_COUNTS    # Array of instance counts

for file in "${ALL_INSTANCE_FILES[@]}"; do
    # Extract repo info from path: data/<lang>/<owner>/<repo>/instances_versions.jsonl
    REL_PATH="${file#$DATA_DIR/}"
    OWNER=$(echo "$REL_PATH" | cut -d'/' -f1)
    REPO=$(echo "$REL_PATH" | cut -d'/' -f2)
    REPO_FULL="$OWNER/$REPO"

    OUTPUT_DIR="$OUTPUT_BASE_DIR/$LANG_LOWER/$OWNER-$REPO"
    RESULTS_DIR="$OUTPUT_DIR/results"
    RESULTS_FILE="$RESULTS_DIR/results.json"

    # Check if already processed
    if [ "$SKIP_EXISTING" = true ] && [ -f "$RESULTS_FILE" ]; then
        echo "[SKIP] $REPO_FULL (results already exist: $RESULTS_FILE)"
        continue
    fi

    # Count instances
    INSTANCE_COUNT=$(wc -l < "$file" 2>/dev/null || echo "0")

    REPO_LIST+=("$REPO_FULL")
    INSTANCE_FILES+=("$file")
    OUTPUT_DIRS+=("$OUTPUT_DIR")
    INSTANCE_COUNTS+=("$INSTANCE_COUNT")
done

TOTAL_REPOS=${#REPO_LIST[@]}

if [ "$TOTAL_REPOS" -eq 0 ]; then
    echo "No repositories to process (all may be filtered or already completed)."
    exit 0
fi

# ============================================================
# 8. Apply start-from / end-at slicing
# ============================================================
if [ "$END_AT" -eq -1 ]; then
    END_AT=$TOTAL_REPOS
fi
if [ "$START_FROM" -ge "$TOTAL_REPOS" ]; then
    echo "start-from ($START_FROM) >= total repos ($TOTAL_REPOS). Nothing to do."
    exit 0
fi
if [ "$END_AT" -gt "$TOTAL_REPOS" ]; then
    END_AT=$TOTAL_REPOS
fi

NUM_TO_PROCESS=$((END_AT - START_FROM))

echo "=========================================="
echo "  Processing Range"
echo "=========================================="
echo "  Total repos found:  $TOTAL_REPOS"
echo "  Processing range:   [$START_FROM, $END_AT)"
echo "  Repos to process:   $NUM_TO_PROCESS"
echo "=========================================="
echo ""

# ============================================================
# 9. Summary log
# ============================================================
SUMMARY_FILE="$OUTPUT_BASE_DIR/${LANG_LOWER}_stage2_summary.jsonl"
mkdir -p "$OUTPUT_BASE_DIR"

# ============================================================
# 10. Progress tracking variables
# ============================================================
BOLD='\033[1m'
DIM='\033[2m'
GREEN='\033[32m'
YELLOW='\033[33m'
RED='\033[31m'
CYAN='\033[36m'
BLUE='\033[34m'
RESET='\033[0m'

PIPELINE_START_TIME=$(date +%s)
PROCESSED=0
SUCCEEDED=0
FAILED=0
SKIPPED=0

format_duration() {
    local secs=$1
    if [ "$secs" -lt 60 ]; then
        echo "${secs}s"
    elif [ "$secs" -lt 3600 ]; then
        echo "$((secs / 60))m$((secs % 60))s"
    else
        echo "$((secs / 3600))h$(( (secs % 3600) / 60 ))m"
    fi
}

draw_progress_bar() {
    local current=$1 total=$2 width=${3:-40}
    local pct=0
    if [ "$total" -gt 0 ]; then
        pct=$(( current * 100 / total ))
    fi
    local filled=$(( current * width / total ))
    local empty=$(( width - filled ))
    local bar=""
    for ((i=0; i<filled; i++)); do bar+="█"; done
    for ((i=0; i<empty;  i++)); do bar+="░"; done
    echo -ne "  ${CYAN}${bar}${RESET} ${BOLD}${pct}%${RESET} (${current}/${total})"
}

print_status_line() {
    local elapsed=$(( $(date +%s) - PIPELINE_START_TIME ))
    echo -e "  ${GREEN}success=$SUCCEEDED${RESET}  ${RED}failed=$FAILED${RESET}  ${YELLOW}skipped=$SKIPPED${RESET}  ${DIM}elapsed=$(format_duration $elapsed)${RESET}"
}

# ============================================================
# 11. Main processing loop
# ============================================================
for (( idx=START_FROM; idx<END_AT; idx++ )); do
    REPO_FULL="${REPO_LIST[$idx]}"
    INSTANCE_FILE="${INSTANCE_FILES[$idx]}"
    OUTPUT_DIR="${OUTPUT_DIRS[$idx]}"
    INSTANCE_COUNT="${INSTANCE_COUNTS[$idx]}"

    RESULTS_DIR="$OUTPUT_DIR/results"
    RESULTS_FILE="$RESULTS_DIR/results.json"

    PROCESSED=$((PROCESSED + 1))
    REPO_START_TIME=$(date +%s)

    # Progress bar
    echo ""
    draw_progress_bar "$PROCESSED" "$NUM_TO_PROCESS"
    echo ""
    print_status_line
    echo -e "  ${BOLD}>>> [$PROCESSED/$NUM_TO_PROCESS] $REPO_FULL${RESET}  ${DIM}($INSTANCE_COUNT instances)${RESET}"
    echo -e "      Input:  $INSTANCE_FILE"
    echo -e "      Output: $OUTPUT_DIR"

    # Build memory pool flags
    MEMORY_POOL_FLAGS=""
    if [ "$DISABLE_MEMORY_POOL" = true ]; then
        MEMORY_POOL_FLAGS="--disable-memory-pool"
    fi

    # Run SWE-Builder
    EXIT_CODE=0
    python app/main.py swe-bench \
        --model "$MODEL" \
        --tasks-map "$INSTANCE_FILE" \
        --num-processes "$NUM_PROCESSES" \
        --model-temperature "$MODEL_TEMPERATURE" \
        --conv-round-limit "$CONV_ROUND_LIMIT" \
        --output-dir "$OUTPUT_DIR" \
        --setup-dir "$TESTBED_DIR" \
        --results-path "$RESULTS_DIR" \
        $MEMORY_POOL_FLAGS \
        2>&1 || EXIT_CODE=$?

    REPO_ELAPSED=$(( $(date +%s) - REPO_START_TIME ))

    if [ "$EXIT_CODE" -ne 0 ]; then
        FAILED=$((FAILED + 1))
        echo -e "  ${RED}FAILED${RESET} SWE-Builder error (exit code $EXIT_CODE)  ${DIM}($(format_duration $REPO_ELAPSED))${RESET}"
        echo "{\"repo\": \"$REPO_FULL\", \"status\": \"failed\", \"exit_code\": $EXIT_CODE, \"instance_count\": $INSTANCE_COUNT, \"elapsed_seconds\": $REPO_ELAPSED}" >> "$SUMMARY_FILE"
        continue
    fi

    # Check if results file was generated
    if [ ! -f "$RESULTS_FILE" ]; then
        FAILED=$((FAILED + 1))
        echo -e "  ${RED}FAILED${RESET} No results file generated  ${DIM}($(format_duration $REPO_ELAPSED))${RESET}"
        echo "{\"repo\": \"$REPO_FULL\", \"status\": \"failed\", \"error\": \"no results file\", \"instance_count\": $INSTANCE_COUNT, \"elapsed_seconds\": $REPO_ELAPSED}" >> "$SUMMARY_FILE"
        continue
    fi

    # Count successful results
    SUCCESS_COUNT=$(python3 -c "
import json
try:
    with open('$RESULTS_FILE') as f:
        data = json.load(f)
    success = sum(1 for item in data if item.get('status') == 'success')
    print(success)
except:
    print(0)
" 2>/dev/null || echo "0")

    SUCCEEDED=$((SUCCEEDED + 1))
    echo -e "  ${GREEN}SUCCESS${RESET} $SUCCESS_COUNT/$INSTANCE_COUNT instances processed  ${DIM}($(format_duration $REPO_ELAPSED))${RESET}"
    echo "{\"repo\": \"$REPO_FULL\", \"status\": \"success\", \"instance_count\": $INSTANCE_COUNT, \"success_count\": $SUCCESS_COUNT, \"elapsed_seconds\": $REPO_ELAPSED}" >> "$SUMMARY_FILE"
done

# ============================================================
# 12. Final summary
# ============================================================
TOTAL_ELAPSED=$(( $(date +%s) - PIPELINE_START_TIME ))

echo ""
echo ""
draw_progress_bar "$PROCESSED" "$NUM_TO_PROCESS"
echo ""
echo ""
echo "=========================================="
echo "  Stage 2 Batch Complete"
echo "=========================================="
echo "  Language:       $LANGUAGE"
echo "  Model:          $MODEL"
echo "  Processed:      $PROCESSED repos"
echo -e "  ${GREEN}Succeeded:    $SUCCEEDED${RESET}"
echo -e "  ${RED}Failed:       $FAILED${RESET}"
echo -e "  ${YELLOW}Skipped:      $SKIPPED${RESET}"
echo "  Elapsed:        $(format_duration $TOTAL_ELAPSED)"
echo "  Summary log:    $SUMMARY_FILE"
echo ""
echo "  Output: $OUTPUT_BASE_DIR/$LANG_LOWER/<owner>-<repo>/results/"
echo "=========================================="
